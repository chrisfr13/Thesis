{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import necessary libraries"
      ],
      "metadata": {
        "id": "2ljIBmR7JWLX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAhmvmWGPB0E"
      },
      "outputs": [],
      "source": [
        "!pip install torchtext==0.17.0\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Base\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification"
      ],
      "metadata": {
        "id": "HG4pAia_PpTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fITjWBwMJVJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Donwload stopwords and set devide where we run experiments\n",
        "nltk.download('stopwords')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize tokenizer and get stopwords\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "Kc-7r2qcPtAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Global Parameters for models"
      ],
      "metadata": {
        "id": "yJe4RVNkD74c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_WORDS = 256\n",
        "EPOCHS = 25\n",
        "LEARNING_RATE = 2e-5\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 256\n",
        "NUM_HEADS = 16"
      ],
      "metadata": {
        "id": "jcvWbp_oPw6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models and dataset classes"
      ],
      "metadata": {
        "id": "dfzfBfZXEtUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['label'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, num_layers=4)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn(embeddings)\n",
        "        logits = self.linear(output[:, -1, :])\n",
        "        logits = self.dropout(logits)\n",
        "        return logits\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for name, param in self.rnn.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.orthogonal_(param)  # Orthogonal initialization for LSTM weights\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0)  # Set biases to 0\n",
        "        nn.init.xavier_uniform_(self.linear.weight)  # Xavier initialization for the linear layer\n",
        "        nn.init.constant_(self.linear.bias, 0)       # Initialize biases\n",
        "\n",
        "class BiRNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(BiRNNModel, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, num_layers=4, bidirectional=True)\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Multiply by 2 for bidirectional\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn(embeddings)\n",
        "        logits = self.linear(output[:, -1, :])\n",
        "        logits = self.dropout(logits)\n",
        "        return logits\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for name, param in self.rnn.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.orthogonal_(param)  # Orthogonal initialization for LSTM weights\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0)  # Set biases to 0\n",
        "        nn.init.xavier_uniform_(self.linear.weight)  # Xavier initialization for the linear layer\n",
        "        nn.init.constant_(self.linear.bias, 0)       # Initialize biases\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, num_layers=4, bidirectional=True)\n",
        "        self.linear = nn.Linear(hidden_dim * 4, output_dim)  # hidden_dim * 2 for pooling + hidden_dim * 2 for forward/backward states\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Default initialize with Xavier weights\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, (hn, cn) = self.lstm(embeddings)\n",
        "\n",
        "        # Max pooling over the sequence length dimension (dim=1)\n",
        "        max_pooled_output, _ = torch.max(output, dim=1)  # Max pool along the seq_len dimension\n",
        "        forward_hidden = hn[-2, :, :]  # Last layer forward hidden state\n",
        "        backward_hidden = hn[-1, :, :]  # Last layer backward hidden state\n",
        "\n",
        "        # Concatenate forward and backward hidden states\n",
        "        final_hidden_state = torch.cat((forward_hidden, backward_hidden), dim=1)  # (batch_size, hidden_dim * 2)\n",
        "        combined_representation = torch.cat((final_hidden_state, max_pooled_output), dim=1)  # (batch_size, hidden_dim * 4)\n",
        "\n",
        "        # Apply dropout to the combined representation\n",
        "        combined_representation = self.dropout(combined_representation)\n",
        "\n",
        "        # Apply the linear layer to the combined representation\n",
        "        logits = self.linear(combined_representation)\n",
        "        return logits\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.orthogonal_(param)  # Orthogonal initialization for LSTM weights\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0)  # Set biases to 0\n",
        "        nn.init.xavier_uniform_(self.linear.weight)  # Xavier initialization for the linear layer\n",
        "        nn.init.constant_(self.linear.bias, 0)       # Initialize biases\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, num_layers=2, bidirectional=True)\n",
        "        self.linear = nn.Linear(hidden_dim*4, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)  # (batch_size, seq_len, embedding_dim)\n",
        "        output, hn = self.gru(embeddings)  # output shape: (batch_size, seq_len, hidden_dim * 2)\n",
        "\n",
        "        # Max pooling over the sequence length dimension (dim=1)\n",
        "        max_pooled_output, _ = torch.max(output, dim=1)  # Max pool along the seq_len dimension\n",
        "        forward_hidden = hn[-2, :, :]  # Last layer forward hidden state\n",
        "        backward_hidden = hn[-1, :, :]  # Last layer backward hidden state\n",
        "\n",
        "        # Concatenate forward and backward hidden states\n",
        "        final_hidden_state = torch.cat((forward_hidden, backward_hidden), dim=1)  # (batch_size, hidden_dim * 2)\n",
        "        combined_representation = torch.cat((final_hidden_state, max_pooled_output), dim=1)  # (batch_size, hidden_dim * 4)\n",
        "\n",
        "        # Apply dropout to the combined representation\n",
        "        combined_representation = self.dropout(combined_representation)\n",
        "\n",
        "        # Apply the linear layer to the combined representation\n",
        "        logits = self.linear(combined_representation)\n",
        "        return logits\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for name, param in self.gru.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.orthogonal_(param)  # Orthogonal initialization for LSTM weights\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0)  # Set biases to 0\n",
        "        nn.init.xavier_uniform_(self.linear.weight)  # Xavier initialization for the linear layer\n",
        "        nn.init.constant_(self.linear.bias, 0)       # Initialize biases\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, num_heads, hidden_dim, output_dim):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim=embed_dim) # 30522 is the vocab size of BERT\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        attn_output, _ = self.attention(embedded, embedded, embedded)\n",
        "        hidden = F.relu(self.fc1(attn_output.mean(dim=1)))  # Pooling\n",
        "        return self.fc2(hidden)\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.bert(input_ids=input_ids, attention_mask=attention_mask)['logits']\n",
        "\n",
        "class RoBERTaClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(RoBERTaClassifier, self).__init__()\n",
        "        self.roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.roberta(input_ids=input_ids, attention_mask=attention_mask)['logits']"
      ],
      "metadata": {
        "id": "6gZg7bLrP0Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supporting functions"
      ],
      "metadata": {
        "id": "TFOAFDVDE2fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of trainable parameters in models\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Creates plots for results\n",
        "def create_plots(df):\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    plt.figure(figsize=(16, 8))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    sns.barplot(x='Dataset', y='Test Accuracy', data=df)\n",
        "    plt.title('Test Accuracy by Dataset')\n",
        "    plt.xticks(rotation=90)\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    sns.barplot(x='Dataset', y='Training Time', data=df)\n",
        "    plt.title('Training Time by Dataset')\n",
        "    plt.xticks(rotation=90)\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    sns.barplot(x='Dataset', y='Parameters', data=df)\n",
        "    plt.title('Model Parameters by Dataset')\n",
        "    plt.xticks(rotation=90)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "XeB_zWDlFBTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load & Prepare the Data"
      ],
      "metadata": {
        "id": "MZV74e1tFD8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    csv_files = glob.glob(os.path.join(file_path, '*.csv'))\n",
        "    dataframes = {os.path.basename(file).split('.')[0]: pd.read_csv(file) for file in csv_files}\n",
        "    return dataframes\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        tokens = tokenizer(text.lower())  # Convert to lowercase\n",
        "        #tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]  # Remove stopwords and punctuation\n",
        "        yield tokens\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            tokens = tokenizer(text.lower())  # Convert to lowercase\n",
        "            #tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]  # Remove stopwords and punctuation\n",
        "            yield tokens\n",
        "\n",
        "def collate_batch(batch, max_words, vocab):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y, dtype=torch.long)  # Targets in range [0,1,2,3]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    if max_words == -1:\n",
        "        max_words = max(len(tokens) for tokens in X)\n",
        "    X = [tokens + ([vocab['<PAD>']] * (max_words - len(tokens))) if len(tokens) < max_words else tokens[:max_words] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int64).to(device), Y.to(device)\n",
        "\n",
        "def make_data_loaders(train_dataset, test_dataset, max_words, vocab):\n",
        "    custom_collate_fn = partial(collate_batch, max_words=max_words, vocab= vocab)\n",
        "\n",
        "    num_train = int(len(train_dataset) * 0.95)\n",
        "    split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "    train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
        "    valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "    return [train_loader, valid_loader, test_loader]\n",
        "\n",
        "def prepare_data_loaders(dataframes):\n",
        "    dataset_loaders = {}\n",
        "\n",
        "    for dname, df in dataframes.items():\n",
        "        test_df = df # current df is the test df\n",
        "\n",
        "        test_df['CLASS'] = test_df['CLASS'].replace({'truthful': 0, 'deceptive': 1})\n",
        "        X_test, y_test = test_df['TEXT'], test_df['CLASS']\n",
        "\n",
        "        # Combine datasets - Exclude current df\n",
        "        cross_df = [cur_df for cur_name, cur_df in dataframes.items() if dname != cur_name]\n",
        "        cross_df = pd.concat(cross_df, axis=0, ignore_index=True)\n",
        "\n",
        "        cross_df['CLASS'] = cross_df['CLASS'].replace({'truthful': 0, 'deceptive': 1})\n",
        "        X_train, y_train = cross_df['TEXT'], cross_df['CLASS']\n",
        "\n",
        "        train_df = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
        "        test_df = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
        "        train_dataset = [(label, train_df['TEXT'][i]) for i, label in enumerate(train_df['CLASS'])]\n",
        "        test_dataset = [(label, test_df['TEXT'][i]) for i, label in enumerate(test_df['CLASS'])]\n",
        "\n",
        "        vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\", \"<UNK>\"])\n",
        "        vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "        loaders = make_data_loaders(train_dataset, test_dataset, MAX_WORDS, vocab)\n",
        "        dataset_loaders[dname] = (loaders, vocab)\n",
        "\n",
        "    return dataset_loaders\n",
        "\n",
        "def prepare_data_loadersTransf(dataframes):\n",
        "    dataset_loaders = {}\n",
        "\n",
        "    for dname, df in dataframes.items():\n",
        "        # Test set\n",
        "        test_df = df # current df is the test df\n",
        "        test_df['CLASS'] = test_df['CLASS'].replace({'truthful': 0, 'deceptive': 1})\n",
        "        X_test, y_test = test_df['TEXT'].tolist(), test_df['CLASS'].tolist()\n",
        "\n",
        "        # Train set - Combine datasets - Exclude current df\n",
        "        cross_df = [cur_df for cur_name, cur_df in dataframes.items() if dname != cur_name]\n",
        "        cross_df = pd.concat(cross_df, axis=0, ignore_index=True)\n",
        "        X_train, y_train = cross_df['TEXT'].tolist(), cross_df['CLASS'].tolist()\n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.05)\n",
        "\n",
        "        # Tokenize texts separately after splitting\n",
        "        X_train_enc = tokenizer(X_train, return_tensors='pt', max_length=MAX_WORDS, padding=True, truncation=True)\n",
        "        X_valid_enc = tokenizer(X_valid, return_tensors='pt', max_length=MAX_WORDS, padding=True, truncation=True)\n",
        "        X_test_enc = tokenizer(X_test, return_tensors='pt', max_length=MAX_WORDS, padding=True, truncation=True)\n",
        "\n",
        "        # Create datasets with tokenized data\n",
        "        train_dataset = TextClassificationDataset(X_train_enc, torch.tensor(y_train))\n",
        "        valid_dataset = TextClassificationDataset(X_valid_enc, torch.tensor(y_valid))\n",
        "        test_dataset = TextClassificationDataset(X_test_enc, torch.tensor(y_test))\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "        dataset_loaders[dname] = ([train_loader, valid_loader, test_loader], None) # Vocab is not needed for these models\n",
        "\n",
        "    return dataset_loaders"
      ],
      "metadata": {
        "id": "APsOoRRZFGn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainining & Evaluation"
      ],
      "metadata": {
        "id": "cPZNBxUKFLV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EvaluateModel(model, loss_fn, val_loader, isTransf=False):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds, losses = [], [], []\n",
        "\n",
        "        for batch in val_loader:\n",
        "            # Simple models or Bert/Roberta\n",
        "            if isTransf is False:\n",
        "                X, Y = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                preds = model(X)\n",
        "            else:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                Y = batch['label'].to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                preds = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds = torch.cat(Y_preds)\n",
        "\n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs, patience, isTransf=False):\n",
        "    best_accuracy = 0.0\n",
        "    best_valid_loss = float('inf')\n",
        "    consecutive_no_improvement = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n",
        "\n",
        "            # Move data to the device\n",
        "            if isTransf is False:\n",
        "                X, Y = batch[0], batch[1]\n",
        "                X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                Y_preds = model(X)\n",
        "            else:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                Y = batch['label'].to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                Y_preds = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_fn(Y_preds, Y)\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Limit gradient - Exploding gradient problems, especially in LSTMS/GRUS\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Training time for the epoch\n",
        "        train_time = time.time() - start_time\n",
        "        avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "\n",
        "        # Validation Phase\n",
        "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader, isTransf=isTransf)\n",
        "        valid_accuracy = accuracy_score(valid_actual, valid_preds)\n",
        "\n",
        "        print(f\"Train Loss: {avg_train_loss:.3f} | Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_accuracy:.3f}\")\n",
        "\n",
        "        # Check for improvement\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_accuracy = valid_accuracy\n",
        "            best_valid_loss = valid_loss\n",
        "            consecutive_no_improvement = 0\n",
        "            best_model_state = model.state_dict()  # Save the best model's state\n",
        "        else:\n",
        "            consecutive_no_improvement += 1\n",
        "\n",
        "        # Early stopping condition\n",
        "        if consecutive_no_improvement >= patience:\n",
        "            print(f\"Early stopping after {epoch} epochs. No improvement for {patience} consecutive epochs.\")\n",
        "            break\n",
        "\n",
        "    # Load the best model's state (if available)\n",
        "    if best_model_state:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return train_time\n",
        "\n",
        "def classification_results(classifier, loaders, patience=3, isTransf=False):\n",
        "\n",
        "    # Setup\n",
        "    train_loader, valid_loader, test_loader = loaders\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(classifier.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Eval\n",
        "    mean_training_time = TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS, patience, isTransf=isTransf)\n",
        "    _, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader, isTransf=isTransf)\n",
        "\n",
        "    # Calculate accuracy, precision, recall, F1 score, and balanced accuracy\n",
        "    accuracy = accuracy_score(Y_actual, Y_preds)\n",
        "    precision = precision_score(Y_actual, Y_preds, average='weighted', labels=np.unique(Y_preds))\n",
        "    recall = recall_score(Y_actual, Y_preds, average='weighted', labels=np.unique(Y_preds))\n",
        "    f1 = f1_score(Y_actual, Y_preds, average='weighted', labels=np.unique(Y_preds))\n",
        "    balanced_acc = balanced_accuracy_score(Y_actual, Y_preds)\n",
        "\n",
        "    print(f\"Test Accuracy : {accuracy:.3f} | Precision (Weighted): {precision:.3f} | Recall (Weighted): {recall:.3f} | F1-Score (Weighted): {f1:.3f} | Balanced Accuracy: {balanced_acc:.3f}\\n\")\n",
        "\n",
        "    # print(\"\\nClassification Report : \")\n",
        "    # print(classification_report(Y_actual, Y_preds, target_names=['0', '1']))\n",
        "    # print(\"\\nConfusion Matrix : \")\n",
        "    # print(confusion_matrix(Y_actual, Y_preds))\n",
        "    # print(f'{total_params}, {mean_training_time}, {accuracy}')\n",
        "\n",
        "    return {\n",
        "        'Parameters': count_parameters(classifier),\n",
        "        'Training Time': mean_training_time,\n",
        "        'Test Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'Balanced Accuracy': balanced_acc\n",
        "    }\n",
        "\n",
        "def classification_results_only(classifier, loaders, patience=3, isTransf=False):\n",
        "\n",
        "    # Evaluate\n",
        "    _, _, test_loader = loaders\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    _, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader, isTransf=isTransf)\n",
        "\n",
        "    # Calculate accuracy, precision, recall, F1 score, and balanced accuracy\n",
        "    accuracy = accuracy_score(Y_actual, Y_preds)\n",
        "    precision = precision_score(Y_actual, Y_preds, average='weighted', labels=np.unique(Y_preds))\n",
        "    recall = recall_score(Y_actual, Y_preds, average='weighted', labels=np.unique(Y_preds))\n",
        "    f1 = f1_score(Y_actual, Y_preds, average='weighted', labels=np.unique(Y_preds))\n",
        "    balanced_acc = balanced_accuracy_score(Y_actual, Y_preds)\n",
        "\n",
        "    print(f\"Test Accuracy : {accuracy:.3f} | Precision (Weighted): {precision:.3f} | Recall (Weighted): {recall:.3f} | F1-Score (Weighted): {f1:.3f} | Balanced Accuracy: {balanced_acc:.3f}\\n\")\n",
        "\n",
        "    # print(\"\\nClassification Report : \")\n",
        "    # print(classification_report(Y_actual, Y_preds, target_names=['0', '1']))\n",
        "    # print(\"\\nConfusion Matrix : \")\n",
        "    # print(confusion_matrix(Y_actual, Y_preds))\n",
        "    # print(f'{total_params}, {mean_training_time}, {accuracy}')\n",
        "\n",
        "    return {\n",
        "        'Parameters': count_parameters(classifier),\n",
        "        'Training Time': 0,\n",
        "        'Test Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'Balanced Accuracy': balanced_acc\n",
        "    }\n",
        "\n",
        "def run_experiment(loaders, model_name, patience=3):\n",
        "    results_list = []\n",
        "\n",
        "    for dname, (loaders, vocab) in loaders.items():\n",
        "\n",
        "        print(f'********Training for {dname}...********')\n",
        "        match model_name:\n",
        "            case \"RNN Model\":\n",
        "                model = RNNModel(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, 2).to(device)\n",
        "                isTransf = False\n",
        "            case \"BiRNN Model\":\n",
        "                model = BiRNNModel(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, 2).to(device)\n",
        "                isTransf = False\n",
        "            case \"LSTM Model\":\n",
        "                model = LSTMModel(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, 2).to(device)\n",
        "                isTransf = False\n",
        "            case \"GRU Model\":\n",
        "                model = GRUModel(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, 2).to(device)\n",
        "                isTransf = False\n",
        "            case \"Transformer Model\":\n",
        "                model = TransformerModel(len(vocab), EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, 2).to(device)\n",
        "                isTransf = False\n",
        "            case \"Bert Model\":\n",
        "                model = BERTClassifier(2).to(device)\n",
        "                isTransf = True\n",
        "            case \"Roberta Model\":\n",
        "                model = RoBERTaClassifier(2).to(device)\n",
        "                isTransf = True\n",
        "            case _:\n",
        "                pass\n",
        "\n",
        "        # In case we train only\n",
        "        results = classification_results(model, loaders, patience, isTransf=isTransf)\n",
        "        #torch.save(model.state_dict(),f\"models/{dname}_{model_name}\")\n",
        "\n",
        "        # In case we evaluate results\n",
        "        # model.load_state_dict(torch.load(f\"models/{dname}_{model_name}\"))\n",
        "        # results = classification_results_only(model, loaders, patience, isTransf=isTransf)\n",
        "\n",
        "        results['Dataset'] = dname\n",
        "        results_list.append(results)\n",
        "\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    results_df['Model'] = model_name\n",
        "    print(f\"\\nResults DataFrame for {model_name}:\")\n",
        "    print(results_df)\n",
        "    #create_plots(results_df)\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "ftnnAdj_FI4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run experiments"
      ],
      "metadata": {
        "id": "MkBy36EGFSIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In case we need data from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# This should be replaced by the actual path to the datasets\n",
        "dataframes = load_data('/content/drive/MyDrive/datasets/')\n",
        "\n",
        "# For all models besides Bert/Roberta\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "dataset_loaders = prepare_data_loaders(dataframes)\n",
        "all_results = []\n",
        "\n",
        "MAX_WORDS = 256; LEARNING_RATE = 2e-3; BATCH_SIZE = 64; EMBEDDING_DIM = 100; HIDDEN_DIM = 128;\n",
        "all_results.append(run_experiment(dataset_loaders, \"RNN Model\", patience=3))\n",
        "\n",
        "MAX_WORDS = 256; LEARNING_RATE = 2e-3; BATCH_SIZE = 64; EMBEDDING_DIM = 100; HIDDEN_DIM = 128;\n",
        "all_results.append(run_experiment(dataset_loaders, \"BiRNN Model\", patience=3))\n",
        "\n",
        "MAX_WORDS = 256; LEARNING_RATE = 2e-3; BATCH_SIZE = 64; EMBEDDING_DIM = 100; HIDDEN_DIM = 128;\n",
        "all_results.append(run_experiment(dataset_loaders, \"GRU Model\", patience=3))\n",
        "\n",
        "MAX_WORDS = 256; LEARNING_RATE = 2e-3; BATCH_SIZE = 64; EMBEDDING_DIM = 100; HIDDEN_DIM = 128;\n",
        "all_results.append(run_experiment(dataset_loaders, \"LSTM Model\", patience=3))\n",
        "\n",
        "MAX_WORDS = 256; LEARNING_RATE = 2e-3; BATCH_SIZE = 64; EMBEDDING_DIM = 100; HIDDEN_DIM = 128; NUM_HEADS = 10\n",
        "all_results.append(run_experiment(dataset_loaders, \"Transformer Model\", patience=3))\n",
        "\n",
        "#Configuration Parameters - Bert and Roberta require too many resources, scale down\n",
        "MAX_WORDS = 256; EPOCHS = 5; LEARNING_RATE = 2e-5; BATCH_SIZE = 16\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dataset_loaders = prepare_data_loadersTransf(dataframes)\n",
        "all_results.append(run_experiment(dataset_loaders, \"Bert Model\", patience=3))\n",
        "\n",
        "MAX_WORDS = 256; EPOCHS = 8; LEARNING_RATE = 2e-5; BATCH_SIZE = 16\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dataset_loaders = prepare_data_loadersTransf(dataframes)\n",
        "all_results.append(run_experiment(dataset_loaders, \"Roberta Model\", patience=3))\n",
        "\n",
        "# Collect all DFs\n",
        "all_results_df = pd.concat(all_results, axis=0, ignore_index=True)\n",
        "all_results_df.to_csv('results/cross_domain_full_results.csv')"
      ],
      "metadata": {
        "id": "yL62tCSwFVty"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}